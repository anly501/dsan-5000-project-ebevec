{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature selection with text data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Read data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"C:\\\\Users\\\\erinb\\\\OneDrive\\\\Documents\\\\Georgetown\\\\DSAN 5000\\\\dsan-5000-project-ebevec\\\\data\\\\01-modified-data\\\\data_clean2.csv\")\n",
        "txt = pd.read_csv('C:\\\\Users\\\\erinb\\\\OneDrive\\\\Documents\\\\Georgetown\\\\DSAN 5000\\\\dsan-5000-project-ebevec\\\\data\\\\01-modified-data\\\\data_syria_switzerland_clean.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Reformat data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CONVERT Y TO NUMPY ARRAY\n",
        "txt2=np.array(txt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Vectorize text and reformat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PARAMETERS TO CONTROL SIZE OF FEATURE SPACE WITH COUNT-VECTORIZER\n",
        "# minDF = 0.01 means \"ignore terms that appear in less than 1% of the documents\". \n",
        "# minDF = 5 means \"ignore terms that appear in less than 5 documents\".\n",
        "# max_features=int, default=None\n",
        "#   If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def vectorize(corpus,MAX_FEATURES):\n",
        "    vectorizer=CountVectorizer(max_features=MAX_FEATURES,stop_words=\"english\")   \n",
        "    # RUN COUNT VECTORIZER ON OUR COURPUS \n",
        "    Xs  =  vectorizer.fit_transform(corpus)   \n",
        "    X=np.array(Xs.todense())\n",
        "    #CONVERT TO ONE-HOT VECTORS (can also be done with binary=true in CountVectorizer)\n",
        "    maxs=np.max(X,axis=0)\n",
        "    return (np.ceil(X/maxs),vectorizer.vocabulary_)\n",
        "\n",
        "(x,vocab0)=vectorize(txt,MAX_FEATURES=10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "#swap keys and values (value --> ley)\n",
        "vocab1 = dict([(value, key) for key, value in vocab0.items()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     0\n",
            "0  1.0\n",
            "1  0.0\n",
            "2  0.0\n"
          ]
        }
      ],
      "source": [
        "#RE-ORDER COLUMN SO IT IS SORTED FROM HIGH FREQ TERMS TO LOW \n",
        "# https://stackoverflow.com/questions/60758625/sort-pandas-dataframe-by-sum-of-columns\n",
        "txt3=pd.DataFrame(x)\n",
        "s = txt3.sum(axis=0)\n",
        "txt3=txt3[s.sort_values(ascending=False).index[:]]\n",
        "print(txt3.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Split data\n",
        "\n",
        "Use index based methods (because x matrix will be re-defined repeatedly but want same rows to be used each time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 2]\n",
            "[1]\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "N=x.shape[0]\n",
        "l = [*range(N)]     # indices\n",
        "cut = int(0.8 * N) #80% of the list\n",
        "random.shuffle(l)   # randomize\n",
        "train_index = l[:cut] # first 80% of shuffled list\n",
        "test_index = l[cut:] # last 20% of shuffled list\n",
        "\n",
        "print(train_index[0:10])\n",
        "print(test_index[0:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "(3, 1) (100, 3)\n",
            "(3, 1) (100, 3)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Found input variables with inconsistent numbers of samples: [2, 6]",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\erinb\\Downloads\\feature-selection-text.ipynb Cell 15\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/erinb/Downloads/feature-selection-text.ipynb#X31sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(x),\u001b[39mtype\u001b[39m(txt2))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/erinb/Downloads/feature-selection-text.ipynb#X31sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mshape,txt2\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/erinb/Downloads/feature-selection-text.ipynb#X31sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m (acc_train,acc_test,time_train,time_eval)\u001b[39m=\u001b[39mtrain_MNB_model(x,txt2,i_print\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "\u001b[1;32mc:\\Users\\erinb\\Downloads\\feature-selection-text.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/erinb/Downloads/feature-selection-text.ipynb#X31sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# TRAIN MODEL \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/erinb/Downloads/feature-selection-text.ipynb#X31sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mprocess_time()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/erinb/Downloads/feature-selection-text.ipynb#X31sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(x_train,y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/erinb/Downloads/feature-selection-text.ipynb#X31sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m time_train\u001b[39m=\u001b[39mtime\u001b[39m.\u001b[39mprocess_time() \u001b[39m-\u001b[39m start\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/erinb/Downloads/feature-selection-text.ipynb#X31sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# LABEL PREDICTIONS FOR TRAINING AND TEST SET \u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\naive_bayes.py:745\u001b[0m, in \u001b[0;36m_BaseDiscreteNB.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[39m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    725\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    726\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Fit Naive Bayes classifier according to X, y.\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \n\u001b[0;32m    728\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    743\u001b[0m \u001b[39m        Returns the instance itself.\u001b[39;00m\n\u001b[0;32m    744\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 745\u001b[0m     X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_X_y(X, y)\n\u001b[0;32m    746\u001b[0m     _, n_features \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape\n\u001b[0;32m    748\u001b[0m     labelbin \u001b[39m=\u001b[39m LabelBinarizer()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\naive_bayes.py:578\u001b[0m, in \u001b[0;36m_BaseDiscreteNB._check_X_y\u001b[1;34m(self, X, y, reset)\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_X_y\u001b[39m(\u001b[39mself\u001b[39m, X, y, reset\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    577\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Validate X and y in fit methods.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 578\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, y, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, reset\u001b[39m=\u001b[39;49mreset)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:622\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    620\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    621\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 622\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[0;32m    623\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1164\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1146\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1147\u001b[0m     X,\n\u001b[0;32m   1148\u001b[0m     accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1159\u001b[0m     input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1160\u001b[0m )\n\u001b[0;32m   1162\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[1;32m-> 1164\u001b[0m check_consistent_length(X, y)\n\u001b[0;32m   1166\u001b[0m \u001b[39mreturn\u001b[39;00m X, y\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:407\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    405\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[0;32m    406\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 407\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    408\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    409\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[0;32m    410\u001b[0m     )\n",
            "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [2, 6]"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "\n",
        "def train_MNB_model(X,Y,i_print=False):\n",
        "\n",
        "    if(i_print):\n",
        "        print(X.shape,Y.shape)\n",
        "\n",
        "    #SPLIT\n",
        "    x_train=X[train_index]\n",
        "    y_train=Y[train_index].flatten()\n",
        "\n",
        "    x_test=X[test_index]\n",
        "    y_test=Y[test_index].flatten()\n",
        "\n",
        "    # INITIALIZE MODEL \n",
        "    model = MultinomialNB()\n",
        "\n",
        "    # TRAIN MODEL \n",
        "    start = time.process_time()\n",
        "    model.fit(x_train,y_train)\n",
        "    time_train=time.process_time() - start\n",
        "\n",
        "    # LABEL PREDICTIONS FOR TRAINING AND TEST SET \n",
        "    start = time.process_time()\n",
        "    yp_train = model.predict(x_train)\n",
        "    yp_test = model.predict(x_test)\n",
        "    time_eval=time.process_time() - start\n",
        "\n",
        "    acc_train= accuracy_score(y_train, yp_train)*100\n",
        "    acc_test= accuracy_score(y_test, yp_test)*100\n",
        "\n",
        "    if(i_print):\n",
        "        print(acc_train,acc_test,time_train,time_eval)\n",
        "\n",
        "    return (acc_train,acc_test,time_train,time_eval)\n",
        "\n",
        "\n",
        "#TEST\n",
        "print(type(x),type(txt2))\n",
        "print(x.shape,txt2.shape)\n",
        "(acc_train,acc_test,time_train,time_eval)=train_MNB_model(x,txt2,i_print=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Search-1: Remove features from high to low"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {},
      "outputs": [],
      "source": [
        "##UTILITY FUNCTION TO INITIALIZE RELEVANT ARRAYS\n",
        "def initialize_arrays():\n",
        "    global num_features,train_accuracies\n",
        "    global test_accuracies,train_time,eval_time\n",
        "    num_features=[]\n",
        "    train_accuracies=[]\n",
        "    test_accuracies=[]\n",
        "    train_time=[]\n",
        "    eval_time=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5 50 50 69.085 68.52000000000001\n",
            "10 100 100 72.7825 73.31\n",
            "15 150 150 75.1475 74.78\n",
            "20 200 200 77.4 77.03\n",
            "25 250 250 79.5 79.09\n",
            "30 300 300 80.28 80.05\n",
            "35 350 350 80.74 80.36\n",
            "40 400 400 81.1675 80.78999999999999\n",
            "45 450 450 81.645 81.21000000000001\n",
            "50 500 500 81.855 81.17\n",
            "55 550 550 82.17750000000001 81.66\n",
            "60 600 600 82.3925 81.82000000000001\n",
            "65 650 650 82.8775 82.05\n",
            "70 700 700 83.0475 82.26\n",
            "75 750 750 83.2975 82.39\n",
            "80 800 800 83.3475 82.37\n",
            "85 850 850 83.34 82.61\n",
            "90 900 900 83.6275 82.53\n",
            "95 950 950 83.855 82.76\n",
            "100 1000 1000 84.0225 82.84\n",
            "5 3250 3250 85.8075 84.44\n",
            "10 5500 5500 86.175 84.35000000000001\n",
            "15 7750 7750 86.3725 84.65\n",
            "20 10000 10000 86.7675 84.50999999999999\n"
          ]
        }
      ],
      "source": [
        "# INITIALIZE ARRAYS\n",
        "initialize_arrays()\n",
        "\n",
        "# DEFINE SEARCH FUNCTION\n",
        "def partial_grid_search(num_runs, min_index, max_index):\n",
        "    for i in range(1, num_runs+1):\n",
        "        # SUBSET FEATURES \n",
        "        upper_index=min_index+i*int((max_index-min_index)/num_runs)\n",
        "        xtmp=x[:,0:upper_index]\n",
        "\n",
        "        #TRAIN \n",
        "        (acc_train,acc_test,time_train,time_eval)=train_MNB_model(xtmp,y,i_print=False)\n",
        "\n",
        "        if(i%5==0):\n",
        "            print(i,upper_index,xtmp.shape[1],acc_train,acc_test)\n",
        "            \n",
        "        #RECORD \n",
        "        num_features.append(xtmp.shape[1])\n",
        "        train_accuracies.append(acc_train)\n",
        "        test_accuracies.append(acc_test)\n",
        "        train_time.append(time_train)\n",
        "        eval_time.append(time_eval)\n",
        "\n",
        "# DENSE SEARCH (SMALL NUMBER OF FEATURES (FAST))\n",
        "partial_grid_search(num_runs=100, min_index=0, max_index=1000)\n",
        "\n",
        "# SPARSE SEARCH (LARGE NUMBER OF FEATURES (SLOWER))\n",
        "partial_grid_search(num_runs=20, min_index=1000, max_index=10000)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.4 ('ANLY590')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "2b6082c1c9eef3a910163f232074f2e179e34ed8469dd2c24c723d1290ad549e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
