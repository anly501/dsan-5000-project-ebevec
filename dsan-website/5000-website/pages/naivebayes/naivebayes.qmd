# Naive Bayes
## Introduction
Naive Bayes is a form of supervised machine learning built on the probabilistic principles of Bayesian Inference. Bayes Theory is used to construct a posterior distribution based on prior information and the liklihood distribution. Measures of central tendency and spread can be determined from this process to demonstrate how the posterior distribution is formed, allowing for conclusions about specific outcomes. It is different than frequentist statistics because it uses prior information and predicts an entire distribution instead of a parameter. Naive Bayes is an algorithm that makes a strong assumption about the independence of features. This algorithm can be used to perform classification in machine learning. There are different variants of Naive Bayes, such as Gaussian, Multinomial, and Bernoulli. These variants are built on the assumption that the features invovled follow a specific distribution - Gaussian, Multinomial, or Bernoulli distributions in this example. A Gaussian Naive Bayes algorithm is appropriate if the feature is likely to be normally distributed - height, for example. A Multinomial Naive Bayes algorithm is apporpriate when the feature has a set of outcomes in the sample space and it is frequently used in text classification. Bernoulli Naive Bayes is used when the data is discrete and binary. Classifying sex is an example where this algorithm is suitable.

In  this project, Naive Bayes will be used to classify text data from the Syria / Switzerland text data, and to classify political regime based on freedom indicators. The objective of the text classifier is to find text indicators of freedom or lack of freedom. The objective of the record data classifier is to model the distribution of political regime based on the freedom indicators. 

## Training
The data was split into training, testing, and validation sets. The training set is 60% of the data, the validation and testing sets are each 20% of the data. These splits are neccesary for a classification model. The training set is used to define the parameters of the model. The validatoin set is used to prevent overfitting or underfitting the model. The testing set is used to indicate model performance. This is essenetial to determining that the model is generalizable and adequately models the relationships within the data. This was done using the train_test_split() function from Sci-kit Learn in Python.

## Feature Selection
### Feature Selection for Record Data
Feature selection was condcuted for the record data. The feature scores were [3.73112048e-01, 6.06463981e-01, 2.38799872e+00, 3.13304293e+00, 3.37789416e+00, 8.68159353e+00, 1.02501328e+01, 3.25585323e+01, 5.58537550e+02] for the columns GDP_2019, pop_2019, unemployment_2019	hf_score_2019, pf_religion_2019, ef_score_2019, hf_score_2019, pf_religion_2019, ef_score_2019, respectviely. This indicated that the economic freedom score is the most signficant feature, and the population is the least significant feature. 

### Feature Selection for Text Data
Feature selection was also conducted for the text data. However, the text analysis was inhibited by the vectorizer indicating too many stop words. 

## Naive Bayes
### Navie Bayes for Record Data
A multinomial naive bayes algorithm was employed to classify political regime as either a closed autocracy, an electoral autocracy, an electoral democracy, or a liberal democracy. The performance of the classifier was 0.593. This is a relatively low performance, but considering the low sample size it is sufficient. It is notable that misclassificatoin only occured between closely linked classes - that is, a closed autocracy was not misclassified as a liberal democracy. 

## Naive Bayes for Text Data
A multinomial Naive Bayes algorithm was also run for the text data. However, the text analysis was inhibited by the vectorizer indicating too many stop words.